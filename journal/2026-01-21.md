# 2026-01-21 — Day 2: Diarization

## What happened

Got speaker diarization working with pyannote.audio. Can now identify who's speaking when in a recording.

## Session 1: Setting up pyannote.audio

**Researched current state of pyannote:**
- Discovered `speaker-diarization-3.1` is now "legacy"
- `speaker-diarization-community-1` is the current open-source model (released with pyannote.audio 4.0)
- Key feature: "exclusive" diarization mode simplifies alignment with transcription timestamps

**Hugging Face setup:**
- Accepted model license at huggingface.co
- Created read access token, added `HF_TOKEN` to my `.keys` file (sourced in `.zshrc`)

**Installed pyannote.audio:**
```bash
pip install pyannote.audio
```

**Hit PyTorch 2.6 compatibility issue:**
- PyTorch 2.6 changed `weights_only` default to `True` for security
- pyannote models need certain classes allowlisted to load
- Fixed by adding safe globals for `Specifications`, `Problem`, `Resolution`, `Scope`

**Hit audio format issue:**
- pyannote expects 16kHz audio, our m4a files are 48kHz
- Resampling wasn't working cleanly, caused sample count mismatch
- Fixed by pre-converting to 16kHz mono WAV using ffmpeg

**Added progress hook:**
- Diarization takes ~5 minutes for a 5-minute file (CPU-bound)
- Progress bars now show what's happening: segmentation → speaker_counting → embeddings → discrete_diarization
- Embeddings step is the bottleneck (4+ minutes)

## The diarization output

For our test file, identified 2 speakers across 110 segments:
- SPEAKER_00 dominates (me telling the story)
- SPEAKER_01 appears in short bursts (Arti asking questions)

Example output:
```
[   1.9 -    4.2] SPEAKER_00
[   8.0 -    8.4] SPEAKER_01
[   9.9 -   11.0] SPEAKER_01
...
```

## Tests written

- `test_convert_to_wav_16k_creates_file` — verifies ffmpeg conversion runs
- `test_convert_to_wav_16k_correct_sample_rate` — verifies output is 16kHz mono
- `test_diarize_returns_expected_structure` — integration test (slow)
- `test_diarize_segments_have_valid_timestamps` — integration test (slow)
- `test_diarize_segments_are_chronological` — integration test (slow)

9 tests passing (7 previous + 2 new fast tests).

## Key learnings

**Pickle files:** Python's serialization format. PyTorch models are saved as pickle. Security risk because loading executes code, hence the new `weights_only=True` default.

**PyTorch:** The ML framework underlying pyannote.audio. MLX (Apple's framework) powers our Whisper transcription. Different engines, same concept.

**Progress feedback matters:** A 5-minute silent process violates the "immediate connection" principle. Progress bars are a form of seeing what you're doing.

## Updated principles.md

Added "When to Type vs. When to Generate" section under Human-First, Tools-Assisted:
- Type yourself when learning new patterns or building muscle memory
- Let Claude generate when it's boilerplate following patterns you know
- Claude should always call out what's worth attention in generated code

---

## What's next

Alignment: combine diarization (who spoke when) with transcription (what was said) to produce speaker-labeled transcripts.

The data structures:
- Whisper: `[{"start": 0.0, "end": 3.2, "text": "Once upon a time..."}, ...]`
- Diarization: `[{"start": 1.9, "end": 4.2, "speaker": "SPEAKER_00"}, ...]`

Goal: `SPEAKER_00: Once upon a time...`
