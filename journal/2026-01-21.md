# 2026-01-21 — Day 2: Diarization

## What happened

Got speaker diarization working with pyannote.audio. Can now identify who's speaking when in a recording.

## Session 1: Setting up pyannote.audio

**Researched current state of pyannote:**
- Discovered `speaker-diarization-3.1` is now "legacy"
- `speaker-diarization-community-1` is the current open-source model (released with pyannote.audio 4.0)
- Key feature: "exclusive" diarization mode simplifies alignment with transcription timestamps

**Hugging Face setup:**
- Accepted model license at huggingface.co
- Created read access token, added `HF_TOKEN` to my `.keys` file (sourced in `.zshrc`)

**Installed pyannote.audio:**
```bash
pip install pyannote.audio
```

**Hit PyTorch 2.6 compatibility issue:**
- PyTorch 2.6 changed `weights_only` default to `True` for security
- pyannote models need certain classes allowlisted to load
- Fixed by adding safe globals for `Specifications`, `Problem`, `Resolution`, `Scope`

**Hit audio format issue:**
- pyannote expects 16kHz audio, our m4a files are 48kHz
- Resampling wasn't working cleanly, caused sample count mismatch
- Fixed by pre-converting to 16kHz mono WAV using ffmpeg

**Added progress hook:**
- Diarization takes ~5 minutes for a 5-minute file (CPU-bound)
- Progress bars now show what's happening: segmentation → speaker_counting → embeddings → discrete_diarization
- Embeddings step is the bottleneck (4+ minutes)

## The diarization output

For our test file, identified 2 speakers across 110 segments:
- SPEAKER_00 dominates (me telling the story)
- SPEAKER_01 appears in short bursts (Arti asking questions)

Example output:
```
[   1.9 -    4.2] SPEAKER_00
[   8.0 -    8.4] SPEAKER_01
[   9.9 -   11.0] SPEAKER_01
...
```

## Tests written

- `test_convert_to_wav_16k_creates_file` — verifies ffmpeg conversion runs
- `test_convert_to_wav_16k_correct_sample_rate` — verifies output is 16kHz mono
- `test_diarize_returns_expected_structure` — integration test (slow)
- `test_diarize_segments_have_valid_timestamps` — integration test (slow)
- `test_diarize_segments_are_chronological` — integration test (slow)

9 tests passing (7 previous + 2 new fast tests).

## Key learnings

**Pickle files:** Python's serialization format. PyTorch models are saved as pickle. Security risk because loading executes code, hence the new `weights_only=True` default.

**PyTorch:** The ML framework underlying pyannote.audio. MLX (Apple's framework) powers our Whisper transcription. Different engines, same concept.

**Progress feedback matters:** A 5-minute silent process violates the "immediate connection" principle. Progress bars are a form of seeing what you're doing.

## Updated principles.md

Added "When to Type vs. When to Generate" section under Human-First, Tools-Assisted:
- Type yourself when learning new patterns or building muscle memory
- Let Claude generate when it's boilerplate following patterns you know
- Claude should always call out what's worth attention in generated code

---

## What's next

Alignment: combine diarization (who spoke when) with transcription (what was said) to produce speaker-labeled transcripts.

The data structures:
- Whisper: `[{"start": 0.0, "end": 3.2, "text": "Once upon a time..."}, ...]`
- Diarization: `[{"start": 1.9, "end": 4.2, "speaker": "SPEAKER_00"}, ...]`

Goal: `SPEAKER_00: Once upon a time...`

---

## Session 2: Word-level timestamps and model comparison

### The alignment problem

To label each word with its speaker, we need word-level timestamps from Whisper, not just segment-level. The question: does MLX Whisper support this?

**Answer: Yes.** 
```python
result = mlx_whisper.transcribe(audio_path, word_timestamps=True)
# Each segment now has a "words" array with start, end, word
```

### The discovery: model size matters for child speech

While exploring alignment, we noticed pyannote detected SPEAKER_01 (Arti) speaking at 8-18 seconds, but Whisper's transcript jumped from 4s to 20s with nothing in between.

**Tiny model (default) output for 5-25 seconds:**
```
[19.68 - 22.90] "Why do the fondows and the girls want to be king?"
```
That's me repeating her question. Arti's actual voice: **completely missing**.

**Large model (`whisper-large-v3-turbo`) output for same window:**
```
[ 7.82 -  8.32] ' Dad,'
[ 9.46 -  9.94] ' why'
[ 9.94 - 10.14] ' do'
...
[17.80 - 18.12] ' king?'
```
Arti's entire question captured: "Dad, why do the Fondos and the Goros want to be king?"

**The tiny model heard nothing. The large model heard 12 words.**

### Full comparison

| Aspect | Tiny Model | Large Model |
|--------|-----------|-------------|
| Arti's soft speech | ❌ Missed entirely | ✅ Captured |
| Sanskrit names | "fondows", "girls", "D'You" | "Yudhishthira", "Durioden", "Pandovas" |
| Conversation flow | Partial, missing Arti's responses | Full back-and-forth |
| Arti answering questions | ❌ Not captured | ✅ "Durioden", "Yudhishthira", "The Trasht" |

### Sanskrit name rendering

| Character | Actual Name | Tiny Model | Large Model |
|-----------|-------------|------------|-------------|
| Yudhishthira | युधिष्ठिर | "you this there", "you this still" | "Yudhishthira", "Yudhishthir" |
| Duryodhana | दुर्योधन | "D'You", "D'You're then" | "Durioden", "Duryoden" |
| Pandavas | पाण्डव | "fondows", "bondos" | "Fondos", "Pondos", "Pandovas" |
| Kauravas | कौरव | "Codos", "girls" | "Goros" |
| Pandu | पाण्डु | "Fondo" | "Pando" |
| Dhritarashtra | धृतराष्ट्र | "the thrashed" | "The Trasht" |

The large model isn't perfect ("Goros" instead of "Kauravas") but it's recognizable. The tiny model's "you this there" for Yudhishthira is almost comical — phonetically trying to make sense of sounds it doesn't recognize as a name.

### The silent gap: 8-18 seconds

This is the most striking difference:

| Model | What it heard during Arti's question |
|-------|--------------------------------------|
| Tiny | *absolute silence — no words, no segments* |
| Large | "Dad, why do the Fondos and the Goros want to be king?" (12 words) |

Pyannote knew someone was talking (SPEAKER_01 detected at 8.0, 9.9, 11.7, 14.4, 17.4 seconds). The tiny model just... didn't transcribe it. Ten seconds of a child's voice, gone.

### Arti's voice throughout the recording

The large model captured Arti answering questions and interjecting:

- `[35.7 - 36.5]` "Durioden" — Arti remembering the villain's name
- `[43.6 - 44.4]` "Yudhishthira" — Arti nailing the protagonist's name
- `[137.1 - 138.2]` "The which father?" — Arti asking for clarification  
- `[145.7 - 146.9]` "The Trasht" — Arti's attempt at Dhritarashtra
- `[314.8 - 316.3]` "What was all the reasons?" — Arti asking a follow-up
- `[318.8 - 319.4]` "I forgot" — classic kid moment

None of these appeared as distinct moments in the tiny model output. The conversation felt one-sided. With the large model, you can hear the back-and-forth of a father teaching his daughter.

### Why this matters

The whole point of this project is capturing stories told to Arti. If the transcription can't hear her voice, we lose half the conversation — the half that matters most for understanding how she's engaging with the stories.

### Code changes

Updated `transcribe.py` to accept optional model parameter:
```python
def transcribe(audio_path: str, word_timestamps: bool = False, model: str = None) -> dict:
    kwargs = {"word_timestamps": word_timestamps}
    if model:
        kwargs["path_or_hf_repo"] = model
    result = mlx_whisper.transcribe(audio_path, **kwargs)
    return result
```

Learned the `**kwargs` pattern: build a dict of parameters, only add optional ones if they have values, then unpack with `**` when calling the function.

### The nuance on model size

Big models aren't always the answer. A former colleague's point worth remembering:
- Small models + better data can outperform large models
- Fine-tuning on your domain (child speech, Sanskrit names) might beat general-purpose large
- Cost, latency, and resource usage matter in production

For now: use large model to establish quality baseline. Later: explore if preprocessing or fine-tuning could achieve similar results with smaller models.

### Hallucination note

The large model produced empty segments at the end (`[338.0 - 338.0]` repeated many times). This is a known Whisper behavior when filling silence. Easy to filter: skip segments where `start == end` or text is empty.

---

## What's next

1. Update tests for new `model` parameter
2. Build the alignment function: match word timestamps to speaker segments
3. Output format: speaker-labeled transcript
