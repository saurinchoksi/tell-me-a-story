# 2026-02-08b — Quiet Speech Recovery Experiment

## Session Type
Build — Experiment

## What Happened

### The question

Session 000002 has a known gap at 241.68–242.83s where diarization detected SPEAKER_01 (Arti) but Whisper produced no transcript. From memory: she asked "Who's father had to go away?" — faint but audible. Can we recover it?

### Experiment design

Built `experiments/quiet_speech_recovery.py`. Extracted a 40s window around the gap (220–260s, enough context for Whisper), created 7 audio variants (gain boost at 10/15/20dB, high-pass filter + gain combos), ran Whisper on each.

### First run: initial_prompt trap

First run returned nothing across all 7 variants. Suspicious — investigated the clips and found Whisper was only transcribing the last ~13s of a 40s clip.

Root cause: the `initial_prompt` contained narrative text that closely matched the actual audio ("his father had been made king. And then because his father had to go away..."). Whisper uses `initial_prompt` as "already transcribed" context — when it heard audio matching the prompt, it skipped ahead. Known Whisper behavior, walked right into it.

Fix: switched to vocabulary-only priming ("Mahabharata, Yudhishthira, Pandu, Pandavas...") plus a no-prompt control. This is the same lesson from the initial_prompt experiment but a different manifestation — prompts with narrative overlap cause skipping, not just spelling issues.

### Second run: honest result

14 runs (7 audio × 2 prompt variants). All transcribed the full clip correctly. None recovered Arti's speech in the target window. What appeared as "recovered" was just your words bleeding into the time window boundaries ("huh?" from before, "Yudhishthira's" from after).

The gain/filter variants didn't help because there isn't enough signal to amplify. If it's indistinct at +20dB to human ears, Whisper can't do better.

### Side finding: vocab prompt helps spelling

Consistent across all audio variants:
- No prompt: "Yudister's" 
- Vocab prompt: "Yudhishthira's"

This confirms vocab-only prompting is safe and useful — it guides spelling without causing the skip problem.

## Decisions Made

- **Vocabulary-only for initial_prompt.** Never include narrative text that could overlap with actual audio content.
- **Accept unrecoverable speech.** Some gaps can't be filled from the recording. Mark them honestly rather than pretending they don't exist.

## What We Learned

1. **initial_prompt narrative overlap → Whisper skips matching audio.** Different from the earlier finding about prompt engineering for names. That was about accuracy; this is about entire sections being silently dropped.
2. **Diarization detection ≠ transcription recovery.** Pyannote's task (detect voice activity + assign speaker) is fundamentally easier than Whisper's (decode words). A diarization segment with no transcript coverage is a legitimate "someone spoke but we can't tell what" signal.
3. **Gain/filtering can't create information.** If the original signal-to-noise ratio is too low, amplification just amplifies noise equally.
4. **Mic placement matters more than post-processing.** For the ESP32 device, proximity to Arti is the real fix.

## Artifacts

- `experiments/quiet_speech_recovery.py` — reusable for other gaps
- `experiments/verify_clip.py` — clip verification utility  
- `experiments/results/quiet_speech_recovery/` — full results + audio clips

## What's Next

- Brainstorm other recovery approaches (different models, spectral methods?)
- Future: `_unrecovered_speech` marker for diarization segments with no transcript coverage
- ESP32 mic placement considerations
