# 2026-01-26 — Day 7: Pipeline Testing + Ingest Design

## What happened

Tested the pipeline on two new real recordings and designed the file ingest/rename system.

## Pipeline testing

Ran `python src/pipeline.py` on:
- `New Recording 63.m4a` — 5.6 min bedtime story conversation
- `New Recording 67.m4a` — 4.8 min continuation

**Results:**
- Speaker separation worked well (SPEAKER_00 = Choksi, SPEAKER_01 = Arti)
- Arti's quiet voice captured in many places
- `[unintelligible]` markers appeared at end of Recording 67 where speech faded out
- A few UNKNOWN fragments remain (trailing fragments not handled by current heuristic)
- Word-level timestamps preserved in JSON

**Sanskrit names:** Whisper struggles as expected:
- "Yudhishthira" — sometimes correct, sometimes "Udister"
- "Pandavas" → "Fondos", "Bondos", "Pondos", "Pando's"
- "Kauravas" → "Goros", "Koros"
- "Duryodhana" → "Turioden", "Duryodin", "Duryodhan"
- "Dhritarashtra" → "The Trasht"

This confirms the Sanskrit correction problem is real and will need addressing.

## Ingest design

**Problem:** Phone recordings come in as "New Recording 63.m4a" — need standardized naming.

**Solution decided:**
1. Check if filename already matches `YYYYMMDD-HHMMSS.m4a` → ESP32 case, leave alone
2. Otherwise rename using file's creation timestamp
3. Detect device from iOS Voice Memos `©too` metadata tag
4. Store in JSON: `original_filename`, `device_source`, `device_detail`, `recorded_at`

**Discovery:** iOS Voice Memos embeds device info:
```
©too: ['com.apple.VoiceMemos (iPad Version 26.2 (Build 25C56))']
```

**Where:** Inside `pipeline.py` at start of `run_pipeline()` — one command does everything.

## Review materials created

For manual audio-vs-transcript comparison:
- `sessions/review-notes-2026-01-25.md` — checklist with timestamps to verify
- `sessions/review-63-transcript.txt` — simple transcript for side-by-side listening
- `sessions/review-67-transcript.txt` — same for second recording

## Spots to verify by listening

**Recording 63:**
- 44-136s: 92-second monologue — did Arti interject?
- 242.8s: UNKNOWN "to." — trailing fragment?
- 334s: Garbled "Thank Thank you" ending

**Recording 67:**
- 3-35s: "Why? Why? Why?" opening — real or hallucination?
- 69.9s: UNKNOWN "would do" — who said it?
- 272-286s: Unintelligible section — actual speech or silence?

## What's next

1. Listen to recordings, compare against transcripts
2. Based on findings, decide if pipeline needs adjustments
3. Build ingest/rename functionality with device detection
4. Later: Sanskrit name correction system

---

*Pipeline working on real data. Now validating output quality.*
