# 2026-02-06 — Normalization Pipeline Complete

## Session Type
Design + build + Code handoffs (3 rounds)

## What Happened

Big session. Started by reviewing Code's completed `src/normalize.py` (LLM module) — clean, no changes needed. Then shifted into building the Mahabharata reference library and designing the full normalization pipeline.

### Mahabharata Reference Library
Built `data/mahabharata.json` — 56 entries organized by category: Pandava family, Kaurava side, elders, divine figures, warriors, allies, groups, places, concepts. Each entry has canonical name, transliteration variants, aliases, and description.

Key design distinction: **variants** are misspellings that get corrected ("Duryodhan" → "Duryodhana"). **Aliases** are legitimate alternate names that don't get corrected ("Partha" stays "Partha"). This keeps the dictionary from "correcting" names I'd actually use when telling stories.

Claude Code's interview caught overlapping entries (Kripa, Dronacharya, Pavana in both variants and aliases). Cleaned the data before Code built the module — fix the data, don't add logic to handle messy data.

Added 8 gods at my request: Dharma (not Yama — for storytelling Dharma is the right name), Vayu, Indra, Surya, Ashvini Kumaras, Vishnu, Shiva, Hanuman. Also added Amba and Shikhandi — the Bhishma's downfall story Arti will hear eventually.

### Architecture Evolution
Started with: separate `normalization.json` artifact per session. Ended with: inline corrections on the transcript itself.

The conversation that got us there:
1. What if I want human corrections too? → Same file, different `_corrected_by`
2. But what about instance-specific corrections ("The" → "Them")? → Position-based, using timestamps
3. Where do corrections live? → In the transcript, with `_original` for audit
4. But normalization is universal (dictionary) vs per-session (LLM)? → Apply both inline, simpler than split brain
5. What about chained corrections (Whisper → LLM → dictionary)? → `_corrections` chain format
6. Is modifying the transcript bad architecture? → Not if you track what's been done. `_processing` metadata.

The final architecture: transcript is a living document enriched through pipeline stages. Each stage adds information, never destroys. This extends naturally to human corrections and hallucination marking later.

### Three Code Handoffs
1. **Dictionary module** (`src/dictionary.py`) — 16 tests, 77 total passing
2. **Pipeline integration** (first attempt) — old architecture with separate `normalization.json`. Cancelled before Code executed.
3. **Pipeline integration** (redesigned) — inline corrections with `_corrections` chain, `_processing` metadata. `src/corrections.py` as pure helper module. 9 new tests, 86 total passing.

## Key Decisions

- **Inline corrections, not separate artifact.** The transcript tells the whole story.
- **Both passes in pipeline.** LLM (per-session, context-dependent) and dictionary (universal) both apply inline.
- **`_original` = Whisper, always.** Set once on first correction, never overwritten. Full chain in `_corrections`.
- **`_processing` metadata.** Tracks what stages ran, their status, model/version, correction counts. You look at the transcript and know its state.
- **Dictionary at query time → dropped.** Everything inline. If dictionary updates, rerun on old transcripts.

## Reflection

The architecture conversation was the most valuable part of the session. The handoff had to be rewritten because the design evolved through dialogue — "what if I want to correct a word?" → "what about 'The' to 'Them'?" → "where do corrections live?" Each question revealed something the original design didn't handle. The final architecture is simpler and more extensible than what we started with.

The Mahabharata library was satisfying to build. 56 entries covering the territory Arti will explore as she grows up with these stories. It's both a normalization tool and the foundation of the story bible.

## What's Next
- Run the pipeline on actual audio — see normalization in action
- Review output for accuracy
- After validation: story element extraction, story bible
