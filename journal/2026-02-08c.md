# 2026-02-08c — Experiments Continued + Autonomous Code Workflow

## Session Type
Build — Experiment + Workflow evolution

## What Happened

### More failed experiments (Desktop-driven)

Continued quiet speech recovery from 2026-02-08b. Ran two more experiments before changing approach entirely.

**Spectral subtraction + bandpass (experiment 2).** Hypothesis: room noise has a consistent spectral profile; subtracting it should improve real SNR unlike flat gain. Profiled noise from a nearby silent gap (236.97–238.68s), applied spectral subtraction plus child-frequency bandpass (200Hz–4kHz). 10 variants × 2 prompts = 20 runs. No recovery.

Key finding: baseline SNR was already 30.3dB. The problem was never noise masking signal. Spectral subtraction actually *decreased* SNR slightly (Δ-0.6dB) by removing signal along with noise. Arti's voice is genuinely very quiet, not noise-masked. Confirmed by listening test — boosted spectral+bandpass clip has faint audible speech at target region, but below intelligibility.

**Whisper decode parameter sweep (experiment 5).** Tested `condition_on_previous_text=False`, `no_speech_threshold` up to 0.99, `hallucination_silence_threshold`, `logprob_threshold=-2.0`, all combined. 10 param combos × 2 audio clips = 20 runs. Completely flat — identical results across every variant.

This conclusively exhausted Whisper-based approaches. The model isn't on a decision boundary for this region; it genuinely doesn't detect speech energy regardless of decode parameters.

### The pivot: autonomous Code exploration

Three Desktop-driven experiments failed. Source separation (experiment 3) was scripted but unrun. Rather than continuing to hand-hold each experiment, decided to test a new workflow: give Claude Code the full context and let it explore autonomously.

Wrote an open-ended SYNC task with:
- Complete history of what failed and why
- Success criteria at multiple levels (ASR recovery > partial recovery > human-audible cleanup)
- A starter list of ideas but explicitly "not exhaustive — add your own"
- "When to stop" defined by exhausting ideas, not hitting a number
- Branch isolation (`experiment/quiet-speech-recovery`) for safety

This is an experiment within an experiment — testing whether Code can handle creative, open-ended problem-solving rather than just executing well-specified tasks.

### Workflow insights crystallized

Several decisions about Desktop ↔ Code coordination:

**Execution mode as a standard SYNC field.** Each task now specifies "run directly" or "plan mode first" with reasoning. General rule: if SYNC has clear context and constraints, skip plan mode. If the task involves touching existing code in multiple places and you want to sanity-check Code's interpretation, use plan mode as a gate.

**When plan mode is redundant.** When SYNC already contains the plan AND the task is open-ended (decisions depend on prior results), plan mode just adds a confirmation step for something that can't be fully planned upfront. This task is both — so: run directly.

**Model selection for autonomous work.** Opus for tasks requiring genuine creativity — deciding what to try next based on what failed, drawing from different fields, knowing when an approach is fundamentally different vs. a variant. Sonnet for well-specified execution.

**`--dangerously-skip-permissions` for contained autonomous tasks.** When the task is isolated to a subdirectory, doesn't touch production code, and runs on a branch, the permission prompts just create babysitting overhead that defeats autonomy. Use it when failure is safe.

## Decisions Made

- **Autonomous Code as a workflow option.** Not the default — reserved for contained exploration where failure is safe and the task benefits from creative iteration.
- **Execution mode on every SYNC task.** Added to memory so Desktop advises consistently.
- **Branch isolation standard for autonomous tasks.** Always.
- **Commit periodically instruction.** Part of the SYNC template for long-running tasks so progress survives session drops.

## What We Learned

1. **SNR ≠ intelligibility.** 30.3dB SNR sounds great on paper. The voice is just genuinely quiet. No amount of noise removal creates signal that isn't there.
2. **Whisper has a hard floor, not a soft one.** Decode parameters don't help because the model isn't marginal on this region — it's confidently silent. Different from hallucination detection where thresholds matter.
3. **Autonomous tasks need "when to stop" defined by idea exhaustion, not step counts.** First draft said "5 approaches" — Code would have stopped there. Revised to "keep going as long as you have hypotheses."
4. **The quality of the handoff determines the quality of the autonomy.** Tight context + loose approach = good autonomous work. Loose context + tight approach = micromanagement that happens to use an LLM.

## Artifacts

- `experiments/spectral_recovery.py`
- `experiments/whisper_params_recovery.py`
- `experiments/results/spectral_recovery/`
- `experiments/results/whisper_params_recovery/`
- `experiments/quiet-speech-recovery-tracker.md` — updated with all results
- Updated SYNC.md with autonomous task template

## What's Next

- Wait for Code to finish autonomous exploration
- Review git log for *how* it worked, not just whether it succeeded
- Process results via `!BUILD` when Code writes to "From Code"
- Apply autonomous workflow pattern to build log automation when ready
