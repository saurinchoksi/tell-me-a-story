# 2026-01-29 — Heuristics Dead End, Pivot to LLM

## Summary
Spent the day chasing time-based heuristics for speaker assignment, hit a wall, reverted everything. The right solution is intelligence, not thresholds. Researched DiarizationLM, decided on Ollama approach for now.

## The Journey

**Problem:** After stripping alignment to raw diarization, many words showed as UNKNOWN — falling in micro-gaps between diarization segments. Words like "and", "the", "why" fragmenting sentences.

**First attempt:** Built `assign_speaker_to_small_gap_words()` — if same speaker on both sides of gap AND gap ≤ 0.5s, assign speaker. Tested on Recording 63_short.

**Result:** Only fixed ONE word ("want"). Most gaps were 0.6-1.1s.

**Second attempt:** Raised threshold to 1.25s to catch more gaps.

**The smell:** This is arbitrary. We're tuning to one recording's gap distribution. Not principled.

**The real insight:** The "uh-huh" (toddler's backchannel) sits in a 2.6s gap — but diarization shows SPEAKER_01 on both sides. Pyannote missed the speaker change entirely. The threshold is doing all the protection work, not the "same speaker" check.

Time thresholds can't distinguish:
- "uh-huh" (listener backchannel → different speaker)
- "and the Pandavas" (sentence continuation → same speaker)

Only semantic context can.

## Decision: Revert and Pivot

Reverted all gap-filling code. Back to clean diarization-only pipeline.

Next: Design LLM post-processing for speaker assignment. Reference DiarizationLM (Google, 2024) — 55% WDER reduction using semantic context.

## DiarizationLM Research

**Key insight:** DiarizationLM is *post-processing*, not diarization. The name is misleading.

Flow:
1. Pyannote → acoustic diarization (voice characteristics)
2. Whisper → transcription (words)
3. DiarizationLM → reads combined output, uses semantic understanding to fix speaker errors

It's a fine-tuned LLaMA 2 8B model. The *technique* is: feed transcript with speaker labels to LLM, ask it to correct assignments based on conversational logic.

**Platform constraints:**
- DiarizationLM expects NVIDIA CUDA — won't run on Mac without GGUF conversion (nobody has done this)
- Jetson Orin Nano has CUDA — could run the actual model later
- Mac now: Use Ollama with general LLM (Llama 3.2) + good prompting

**Decision:** Use Ollama for now. Learn the technique. Migrate to DiarizationLM on Jetson when that hardware comes online.

## Principle Reinforced

**Use intelligence, not heuristics.**

The temptation with ML pipelines is to stack heuristics. Each one feels like progress. But they're brittle, arbitrary, and don't generalize. Better to wait for the right tool (LLM) than accumulate technical debt.

## Technical State
- 38 tests passing
- No gap-filling heuristics
- Pipeline: words → filter → align_to_speakers → words_to_utterances → consolidate
- Debug output still available via `--debug`

## Next
Design Ollama-based speaker assignment post-processing. Questions to answer:
- Input/output format for the LLM
- Model size that works well locally
- How to handle long transcripts exceeding context window
- Integration point in pipeline (separate step vs. integrated)
