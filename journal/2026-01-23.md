# 2026-01-23 — Day 4: Leading Fragments

## What happened

Tackled the remaining 7 UNKNOWN utterances from yesterday's alignment pipeline. Got them down to 1.

## The pattern

Looked at the 7 UNKNOWNs and found a clear pattern:

| Text | Gap to next | Actual speaker |
|------|-------------|----------------|
| "Okay," | 0.4s | SPEAKER_00 (me) |
| "That's" | 0.0s | SPEAKER_00 — start of "That's right" |
| "The" | 0.0s | SPEAKER_00 — start of "The blind father" |
| "The? The" | 0.0s | SPEAKER_01 — Arti trying to say "The Trasht" |
| "I" | 0.0s | SPEAKER_00 — start of "I just told you" |
| "He knows." | 0.7s | Hallucination |
| "He knows..." | (end) | Hallucination |

The real ones (not hallucinations) are all **leading fragments** — the start of someone's turn that diarization missed. They have tiny or zero gaps to the next utterance because they're literally the beginning of the same speech.

## The fix

Added `assign_leading_fragments()` — if an UNKNOWN ends within 0.5s of when the next utterance starts, assign it to that next speaker.

```python
def assign_leading_fragments(utterances: list[dict], max_gap: float = 0.5) -> list[dict]:
    for i, utt in enumerate(utterances):
        if utt["speaker"] is None and i < len(utterances) - 1:
            next_utt = utterances[i + 1]
            gap = next_utt["start"] - utt["end"]
            
            if gap <= max_gap and next_utt["speaker"] is not None:
                # Assign to next speaker
```

This is more principled than just "assign to whoever comes next" — we're saying "this is clearly part of the same speech event."

## Pipeline now

```
align_words_to_speakers
    → group_words_by_speaker
    → merge_unknown_utterances (fills between same speaker)
    → assign_leading_fragments (fills turn-starts) ← NEW
    → consolidate_utterances
```

Result: 75 → 1 UNKNOWN (down from 75 → 7 yesterday)

## The remaining UNKNOWN

```
[ 337.2 -  337.9] UNKNOWN: again tomorrow. Thank you.
```

This is at the end of the file with no next utterance to assign to. Also likely a hallucination — Whisper heard Arti say something quiet about her blanket and guessed "again tomorrow. Thank you." because it fits the context of me saying "we can talk about it tomorrow."

Interesting: yesterday this was "He knows. He knows. He knows." — random repetition hallucination. Today it's a contextual hallucination that sounds plausible. Both are wrong, but different failure modes.

## Hallucination types observed

1. **Random repetition** — "He knows. He knows. He knows." — happens with silence/noise
2. **Contextual filling** — "again tomorrow" — happens with quiet speech, model guesses something coherent

The second is sneakier because it sounds right. Future work: confidence thresholds, audio energy checks.

## Model clarification

Cleared up confusion about what models we're using:

- **Whisper** (MLX Whisper large-v3-turbo) — speech recognition, audio → words
- **Pyannote** (speaker-diarization-community-1 from Hugging Face) — speaker diarization, audio → speaker segments

Both are specialized audio models, neither is an LLM. Pyannote is the library, the model is hosted on Hugging Face.

A better diarization model might eliminate some of our post-processing. Our gap-filling and fragment-assignment code is patching model weaknesses.

## Tests

7 new tests for `assign_leading_fragments`:
- Basic assignment within gap
- Gap too large stays UNKNOWN
- UNKNOWN at end stays UNKNOWN  
- Next utterance also UNKNOWN — don't chain
- Empty input
- No UNKNOWNs (passthrough)
- Custom threshold parameter

27 total fast tests, all passing.

## What's next

- Hallucination filtering (confidence scores? audio energy?)
- Save transcripts to file
- Story element extraction
