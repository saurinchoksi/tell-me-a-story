# 2026-02-05 — Validator Filter Integration + Fix

## Summary
Wired probability filter into the validator tool. First attempt used word-level filtering (remove individual words below threshold), which stripped valid words from good segments. Fixed to segment-level filtering (keep or drop entire segments).

## What Happened

### Filter Integration (via Code)
- Server: `?min_prob=` query param on `/transcript/<stem>` endpoint
- UI: "Hide Low Confidence" toggle button with `F` keyboard shortcut
- Notes panel: filtered-out segments show "HIDDEN" badge, disabled Jump/Edit
- Notes migrated from `segmentIndex` to `segmentId` for stability across filtering

### The Bug: Word-Level Filtering Strips Good Words
Tested with the toggle on. "Dad," disappeared from Segment 1, "Why" disappeared from Segment 2. Both are real words — just low confidence. The 0.5 threshold applied per-word is too blunt.

### The Fix: Segment-Level Inclusion
Changed the question from "does this word pass?" to "does this segment have ANY word that passes?" If even one word in a segment is above threshold, keep the entire segment intact. Drop only when every word fails.

**Why this works:** Hallucinations are entire segments of garbage — a single fabricated word with nothing else ("Right." at 0.086). Real speech has a mix of confident and uncertain words. The segment is the right unit.

**What we still lose:** 3 segments where I'm saying Dhritarashtra and Whisper mangled it to "The..." — every word is low-confidence because Whisper couldn't decode any of it. These are the `[unintelligible]` cases. Raw artifacts still have them; toggle off to see them.

### SYNC Processing
- Processed two handoffs from Code (filter integration + fix)
- Both moved to SYNC-LOG.md
- SYNC.md cleared

## Key Insight
The unit of filtering matters. Word-level and segment-level filtering look similar in code but produce very different results. Hallucinations are segment-shaped problems, not word-shaped problems.

---

## Sanskrit Name Experiment (Evening Session)

### The Problem
Whisper mangles transliterated Sanskrit names: "Pandos" (spoken) → "fondos" (transcribed), "Kauravas" → "goros", "Yudhishthira" → "Yudister". Need canonical forms for story bible searchability.

### Research Phase
Deep dive into Whisper's `initial_prompt` parameter:
- Works via token prepending (fake "previous transcript" context)
- 224 token limit, position matters (vocabulary at end more effective)
- MLX Whisper fully supports it identically to OpenAI
- `carry_initial_prompt` does NOT exist in MLX Whisper (verified in source)
- Beam search NOT implemented in MLX Whisper (discovered at runtime)

### Experiment: 3 Runs
| Run | Prompt | Result |
|-----|--------|--------|
| 0 | None (baseline) | fondos, goros, Yudister |
| 1 | Vocab list | Pandus, Kaurava, Yudhishthira |
| 2 | Natural sentence | Pandava, Kaurava, Yudhishthira |

### The Insight
Vocab list gave **phonetic accuracy** — transcribed what I actually say ("Pandos" → "Pandus"). Natural sentence biased toward **canonical forms** ("Pandos" → "Pandava"). But both approaches "lie" about what was spoken.

### Decision: Post-Processing Normalization (Path 2)
Preserve honest transcripts, add `canonical` field:
```json
{
  "word": "fondos",
  "canonical": "Pandavas",
  "start": 10.22,
  "end": 10.7,
  "probability": 0.001
}
```

Aligns with project principle: honest transcripts matter more than clean ones.

## What's Next
1. **LLM normalization experiment** — run on baseline transcript, measure canonicalization accuracy
2. LLM speaker assignment — DeepSeek R1 8B ready to integrate
3. Manual validation — test the fixed filter, confirm behavior
