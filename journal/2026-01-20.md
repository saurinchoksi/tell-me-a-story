# 2026-01-20 — Day 1: Starting

## What I'm building

Tell-Me-A-Story: A local story capture pipeline. Record and input bedtime stories with Arti. Output speaker-labeled transcripts. Build a searchable story bible over time. Gain insights into Arti-focused storytelling and her characters, worlds, themes, etc.

## Why I'm starting with backend

Three layers to this project: device, backend, UI. Backend is the core—if I can't process audio into useful transcripts, nothing else matters. Device and UI are delivery mechanisms. Backend is the engine.

## What "backend" means right now

Smallest version: one audio file goes in, transcript comes out.

Even smaller: can I load an audio file and see what's in it? Duration, format, sample rate. Before any ML, just prove I can work with audio in Python.

## How I want to approach this

- Small steps, not big leaps
- Build tests as I go
- Internalize solid architecture thinking
- Understand the tools well enough to list them as real skills
- Document the journey—this journal

## Build principles

Watched Dan Shipper's "4 Patterns That Work" video. Pulled out principles that resonated—patterns over tools, principles over rules, build with the agent, leave infrastructure doors open. See [principles.md](../docs/principles.md) for the living doc.

## Technologies I expect to use

- pyannote.audio for diarization (separating speakers)
- MLX Whisper for transcription (runs on M1 Mac)
- Python, pytest

## How I'm feeling

Excited. A little nervous. Some uncertainty about whether I'll do this "right." But I'm starting anyway.

---

## Session 1: First code

**Set up project structure:**
- Created `src/`, `tests/`, `data/`, `docs/`, `journal/` directories
- Created `.gitignore` (excludes `data/`, `venv/`, `CLAUDE.md`)
- Wrote `README.md` and `docs/principles.md`

**Set up Python environment:**
- Created virtual environment (`python3 -m venv venv`)
- Installed `mutagen` for audio metadata
- Installed `pytest` for testing

**Wrote first script: `src/inspect_audio.py`**

Loads an audio file and returns its properties (filename, format, duration, sample rate, channels). Refactored to separate "get data" from "print data" so the logic is testable.

**First test file:** `data/New Recording 63.m4a`
- 5.6 minutes (338 seconds) of a storytelling session
- 48kHz sample rate, stereo

**Wrote first tests: `tests/test_inspect_audio.py`**
- Test that valid file returns dict with expected keys
- Test that missing file returns None
- Test that our audio file has expected properties

Ran `pytest` — 3 tests passed.

**Key learning:** Separate concerns early. A function that returns data is testable. A function that prints is not. This small refactor paid off immediately.

---

## Session 2: Process and tools philosophy

Figured out session handoff approach: journal is the handoff doc. Each session updates "What's next" at the bottom. Multiple sessions per day go in the same file, separated by `---`.

**Clarified my approach to Claude Code vs writing code myself:**

Right now I'm writing code with Claude Desktop guiding me. This is intentional. I want to know the libraries, understand what's in the codebase, not feel like it's a black box. This foundation matters.

As patterns stabilize and I'm confident in the architecture, I'll bring in Claude Code for larger tasks. The principle: if the agent builds it, the agent can maintain it. But confidence comes from understanding first.

Updated `docs/principles.md` with "Human-First, Tools-Assisted" section capturing this progression:
1. **Now:** Write code with Claude Desktop guiding. Learn the pieces.
2. **Soon:** Hand well-defined tasks to Claude Code. Stay in the loop.
3. **Later:** Let Claude Code build features while I focus on architecture.
4. **Eventually:** Orchestrate multiple Claude Code sessions—one writing code, one reviewing, one building tests, multiple working on different features in parallel.

The arc of this project: from writing a few lines of code myself to orchestrating a team of AI agents. That's the builder skill I want to develop.

---

## What's next

Transcription. Feed audio file to MLX Whisper and see what comes out.

---

## Session 3: Transcription with MLX Whisper

**Installed mlx-whisper:**
```
pip install mlx-whisper
```
This pulled in MLX framework, torch, huggingface_hub, and other dependencies. First run downloads the default Whisper model (~140MB).

**Wrote `src/transcribe.py`:**
- `transcribe(audio_path)` — Calls `mlx_whisper.transcribe()`, returns result dict
- `save_transcript(result, output_path)` — Writes transcript and timestamped segments to file
- CLI interface that prints to console and saves to `.txt` file

**Ran first transcription on `data/New Recording 63.m4a`:**
- Took ~30 seconds on M1 Mac
- Successfully transcribed 5.6 minutes of Mahabharata storytelling
- Output saved to `data/New Recording 63.txt`

**What Whisper returns:**
- `text` — Full transcript as one string
- `language` — Auto-detected language (`'en'`)
- `segments` — List of dicts with timestamps, text, and confidence scores

Each segment includes:
- `start` / `end` — Timestamps in seconds
- `text` — What was said
- `no_speech_prob` — Probability segment was silence
- `temperature` — Model confidence (0.0 = confident)

**Observations:**
- Whisper phonetically guesses unfamiliar names: "Yudhishthira" → "you this there", "Duryodhana" → "D'You're older", "Pandavas" → "fondos/bondos", "Dhritarashtra" → "the thrashed"
- Timestamps reveal conversation gaps—useful for diarization later
- Short segments (1-3 sec) often indicate dialogue; longer segments (5-10 sec) are explanations

**Key learning:** The segments with timestamps are the foundation for speaker diarization. When we add pyannote.audio, we'll align speaker labels with these time ranges.

**First transcript saved.** The story bible has begun.

---

## What's next

Write tests for `transcribe.py`. Lock in the transcription code before moving to diarization.
