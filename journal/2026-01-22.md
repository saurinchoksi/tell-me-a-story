# 2026-01-22 — Day 3: Alignment Pipeline

## What happened

Built the alignment pipeline that combines transcription (what was said) with diarization (who said it) to produce speaker-labeled transcripts.

## The alignment problem

We have two data sources:
- **Whisper** gives us words with timestamps, but no speaker info
- **Pyannote** gives us speaker segments with timestamps, but no words

The challenge: Whisper might lump "Once upon a time" (me) and "Dad why?" (Arti) into one segment if there's no long pause. So we need word-level alignment.

**The solution:** Use word midpoints. For each word, calculate its midpoint timestamp, find which speaker segment contains that midpoint, assign the speaker.

## The diarization gap problem

Initial output had lots of UNKNOWN speakers. Investigation revealed:
- 109 gaps between 110 diarization segments
- Average gap: 1.1 seconds (not tiny boundary issues)
- Pyannote struggles with child speech — catches fragments, misses chunks

**Tested hypothesis:** When UNKNOWN is sandwiched between same speaker, it's probably that speaker.
- Result: 68 out of 73 UNKNOWNs (93%) were between same speaker
- Built `merge_unknown_utterances` to fill these in
- Reduced UNKNOWNs from 75 → 7

## The fragmentation problem

After merge, consecutive utterances from same speaker were still separate entries. Added `consolidate_utterances` to combine them.
- Reduced utterances from 159 → 23
- Now reads as actual conversation

## Final pipeline

```
audio file
    → transcribe (Whisper large model, word timestamps)
    → diarize (pyannote)
    → align_words_to_speakers (midpoint matching)
    → group_words_by_speaker (combine into utterances)
    → merge_unknown_utterances (fill gaps between same speaker)
    → consolidate_utterances (combine consecutive same-speaker)
    → format_transcript (readable output)
```

## The output

```
SPEAKER_01: Dad, why do the Fondos and the Goros want to be king?
SPEAKER_00: Uh-huh. Well, so the oldest brother of the Goros, his name was, do you remember?
SPEAKER_01: Durioden.
SPEAKER_00: Durioden, yeah. And the oldest brother of the Pondos, his name is?
SPEAKER_01: Yudhishthira.
```

Arti remembering Duryodhana and Yudhishthira. This is what the project is for.

## Key learnings

**Chained comparisons:** Python lets you write `if a <= b <= c` instead of `if a <= b and b <= c`. Reads like English.

**`.extend()` vs `.append()`:** extend unpacks a list and adds items individually (flat), append adds the whole list as one item (nested).

**`result[-1]`:** Python shorthand for "last item in list."

**Simple over clever:** Chose to run a second grouping pass rather than refactor to merge at word level. Redundant but readable beats elegant but tricky.

## Process learnings

**Why go slow when Claude Code could build it all?**

The value isn't becoming a faster coder. It's:
1. **Judgment** — Understanding *why* we use midpoints, *what* diarization gaps mean. When something breaks, knowing where to look.
2. **Directing** — Knowing when to trust the AI and when to question it. Stopping to ask "why do we want a readable string?"
3. **Trade-offs** — Large model catches child speech, diarization struggles with soft voices. This knowledge matters for decisions.

The 2026 builder skill is knowing *when* to go deep and *when* to let the machine run.

## Session flow improvements

Updated `principles.md` with:
- **Session Flow for Learning** — concept → discuss → logic walkthrough → Claude writes → review
- **Understanding vs. Typing** — the value is in the mental model, not the keystrokes
- **Simple Over Clever** — prefer readable over elegant

## Tests

13 new tests for align module, all passing. 20 total fast tests.

## What's next

- Filter hallucinations (the "Guys Guys Guys" and "To sleep" repetitions at end)
- Save transcripts to file
- Start thinking about story element extraction (characters, worlds, plot beats)
