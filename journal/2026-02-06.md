# 2026-02-06 — LLM Normalization: Segment vs Full Transcript Experiment

## The Question

Does segment-by-segment LLM processing catch more phonetic mishearings than full-transcript processing? Or does it lose too much context?

## Setup

- Model: qwen3:8b (local via Ollama)
- Test file: run_0.json (89 segments, ~5 min audio)
- Ground truth: `fondos`, `fondo`, `goros`, `yudister`, `dhrashtra`

## Results

| Approach | Recall | Caught | Missed | False Positives |
|----------|--------|--------|--------|----------------|
| Full transcript | 5/5 (100%) | fondos, fondo, goros, yudister, dhrashtra | — | 0 |
| Segment-by-segment | 4/5 (80%) | fondos, goros, yudister, dhrashtra | fondo* | 3 |

*fondo missed due to timeout, not model failure

**Winner: Full transcript.** Perfect recall, zero false positives, single LLM call.

## Segment-by-Segment Details

### True Positives (in ground truth)
- fondos → Pandavas ✓
- goros → Kauravas ✓
- yudister → Yudhishthira ✓
- dhrashtra → Dhritarashtra ✓

### Bonus Catches
- pondovas → Pandavas (valid mishearing not in original ground truth)

### Hindi Variants (Pass 2 territory, not mishearings)
- duryodhan → Duryodhana
- yudhisthir → Yudhishthira

### False Positives (hallucinations)
- best → Bhishma ❌ (English word)
- dad → Pandu ❌ (English word)
- father → Dhritarashtra ❌ (English word)

## Technical Issues Found

1. **Empty JSON format:** Model sometimes returns `{}` instead of `{"corrections": []}` — regex doesn't match
2. **Timeouts:** Some segments hit 120s timeout
3. **Non-determinism:** Same segment can produce different mappings on different runs (fondos/goros got swapped once)

## Conclusions

1. **Full transcript achieves 100% recall** — caught all 5 ground truth items
2. **Full transcript has 100% precision** — no false positives
3. **Segment-by-segment hallucinates** — maps English words to Sanskrit names

## Architecture Implications

**Full transcript wins definitively.** No hybrid approach needed.

Two viable paths:

**Pass 1: Full Transcript LLM**
- Single call to qwen3:8b
- Catches all phonetic mishearings
- 100% recall, 0 false positives

**Pass 2: Dictionary (Hindi variants)**
- Duryodhan → Duryodhana
- Yudhisthir → Yudhishthira

## Files

- `/experiments/segment_normalization_test.py` — segment test script
- `/experiments/full_transcript_test.py` — full transcript test script
- `/experiments/results/run_0.json` — test transcript
- `/experiments/results/full_transcript_result.json` — full transcript results
- `/experiments/results/llm_normalization_experiment.md` — full experiment writeup

---

## Addendum: Filter Layer Investigation

Noticed "The..." in segments 36-38 in the validation player — hidden by "Hide Low Confidence." Investigated whether these are hallucinations or real speech being filtered out.

**Finding:** NOT hallucinations. Temp 0.0, compression 1.5, no_speech_prob 0.015. Arti is trying to say "Dhritarashtra" after being asked "do you remember his father's name?" Whisper hears "Dhri-" and maps to "The" — low probability (0.32-0.53) but real decoding attempt.

**Pipeline impact:** None. `clean_transcript()` only removes zero-duration words and empty segments. These survive. The `min_probability` filter only runs in the validation player display toggle, not in the pipeline.

**Third category discovered:** Beyond "fabricated" (remove) and "unintelligible" (mark), there's "partially decoded but wrong" — real speech where Whisper produced something at temp 0.0 but got the word wrong. This is exactly what the normalization runner is for. The LLM seeing context ("remember his father's name?" → "The..." → "look up the pronunciation") can infer the intended word.
