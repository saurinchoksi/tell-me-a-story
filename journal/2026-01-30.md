# 2026-01-30 — LLM Speaker Correction Design

## Session Focus

Design session for LLM-based speaker diarization post-processing. Research, model selection, prompt engineering, testing framework planning.

## Key Decisions

**Model selection:** Qwen3 8B as primary candidate. DeepSeek R1 8B and Gemma 3 12B QAT as alternatives for comparison testing.

**Why Qwen3 8B:**
- Hybrid thinking mode (deep reasoning vs fast mode)
- Trained on multi-turn dialogue with human preference alignment
- 128K context window handles long transcripts
- ~5.2GB model, ~10GB RAM — comfortable on 16GB M1

**Why not Kimi K2.5:** Despite being state-of-the-art, requires 250GB+ storage even with extreme quantization. Cloud-only option via Ollama.

**Why not Llama 4:** No 8B variant exists. Smallest is MoE with 17B active parameters (54GB+ VRAM).

## Prompt Design

Four-example few-shot prompt with:
1. Question-answer pattern (speaker switch)
2. Backchannel mid-sentence ("mm-hmm")
3. Listener reaction ("Really?")
4. Single word reassignment (misattributed word)

Features:
- Confidence markers: `<spk:0?>` / `<spk:1?>` for uncertain assignments
- Can leave truly ambiguous words as `<spk:?>` 
- Reasoning output before "CORRECTED:" separator
- Generic "parent and child" framing (not Mahabharata-specific)

## Testing Framework

Single script approach (`scripts/compare_llm_models.py`) — keeps it simple. Extracts to src/ later if needed.

Output: `test_outputs/llm_comparison_YYYY-MM-DD.md` with:
- Input transcript
- Each model's reasoning + corrected output + timing
- Summary comparison table

## Research Notes

**DiarizationLM findings (from yesterday):**
- Zero-shot performs poorly — few-shot examples essential
- 44-55% WDER reduction possible with fine-tuned models
- TPST algorithm handles LLM output fuzziness (dropped/substituted words)

**M1 Mac reality:**
- 8B models: ~10GB RAM, 15-25 tok/sec
- 12B models: ~12-14GB RAM, slower
- 14B models: ~14GB RAM, need to close other apps

## What's Next

Claude Code executes SYNC.md task:
1. Install Ollama + pull models
2. Run comparison script on Recording 63_short
3. Generate comparison document

Then: manual review, create ground truth, select winning model.

---

# Later Session: Whisper Audio Length Investigation

## Problem Discovered

Noticed missing speech in short audio sample transcriptions. Parent's repeated question ("Why do the Pandavas and Kauravas want to be king?") at 20-23s captured in full recording but MISSING in 38s sample.

## Investigation Path

1. **Initial hypothesis:** Code regression from recent commits (hallucination filter, alignment simplification)
2. **Ruled out:** Git history showed no relevant changes that would cause speech drops
3. **Model comparison:** Discovered complementary failures — small model captures segments large model misses, and vice versa
4. **Key breakthrough:** Ran pipeline on BOTH `big_sample.m4a` (full 338s) and `sample.m4a` (38s excerpt)

## Root Cause: Audio Length Dependency

**Same Whisper model (large-v3-turbo), same parameters, same pipeline:**
- Full recording → captures BOTH Arti's question AND parent's repeat ✅
- 38s sample → captures Arti's question, SKIPS parent's repeat ❌

Whisper transcription quality degrades on short audio samples. This is model behavior, not code.

## Technical Details

- Full transcript shows segment at 19.56-22.68s with repeated question
- Short transcript jumps from 18.16s directly to 22.96s ("Uh-huh")
- ~4 second gap where speech is simply not transcribed

## Possible Causes (to research)

1. Whisper's 30-second seek/chunking behavior
2. Context effects — longer recordings provide more audio context
3. Boundary artifacts from sample extraction

## Practical Takeaway

For reliable testing, use full-length recordings. Short sample extraction introduces transcription artifacts. This is valuable knowledge for understanding Whisper's practical limitations.
