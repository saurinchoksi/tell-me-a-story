# 2026-01-25 — Day 6: JSON Output Complete

## What happened

Built `save_session()` to persist pipeline output. The system now captures audio, processes it, and saves structured JSON to `sessions/processed/`. Also did a code review sweep and fixed a couple of bugs.

## The build

**`save_session()` function:**
- Takes pipeline result + audio path
- Extracts unique speakers from utterances
- Generates `story_id` from filename
- Writes JSON with schema version for future migrations

**Schema (v0.1.0):**
```json
{
  "_schema_version": "0.1.0",
  "meta": {
    "source_audio": "path/to/file.m4a",
    "transcribed_at": "2026-01-25T...",
    "pipeline_version": "0.1.0"
  },
  "speakers": {
    "detected": ["SPEAKER_00", "SPEAKER_01"],
    "names": null
  },
  "stories": [{
    "story_id": "filename",
    "utterances": [...]
  }],
  "moments": [],
  "processing": null
}
```

**Design decisions:**
- `stories` is an array — future-proofs for splitting one recording into multiple stories
- `moments` placeholder — for capturing interesting non-story fragments later
- `processing` placeholder — for stats (words filtered, unknowns merged, etc.)
- `speaker_names` placeholder — for mapping SPEAKER_00 → "Choksi", SPEAKER_01 → "Arti"

**Utterances now carry word-level data:**
```json
{
  "speaker": "SPEAKER_00",
  "start": 0.0,
  "end": 3.5,
  "text": "Once upon a time",
  "words": [
    {"word": "Once", "start": 0.0, "end": 0.3, "probability": 0.95},
    ...
  ]
}
```

This enables future caption sync (audio plays, words highlight like Apple Music lyrics).

## Code review cleanup

Ran 5 parallel review agents on all source files. Findings:

**Fixed:**
- `transcribe.py:85` — `sys.exit` wasn't being called (missing parens)
- `pipeline.py` — added file existence check at top of `run_pipeline()`

**Not fixed (by design):**
- Input validation / empty-list checks — callers guarantee valid input, prefer loud failures
- Command injection concern in `diarize.py` — false positive, list form of `subprocess.run()` is safe

## Schema design conversation

Had a good discussion about what to capture. Key principles:

1. **Replay-ability** — if you can't recreate it later, store it now
2. **Cost of retrieval** — if regenerating is expensive (3+ min diarization), cache the result
3. **Regret heuristic** — cheap to store, impossible to recover; bias toward capture

Landed on layered schema:
- **Core** — stories, utterances, words (the reason the project exists)
- **Forensics** — processing stats, pipeline version (future debugging)
- **Future** — extracted elements, speaker names, moments (placeholders)

"Capture generously, build features sparingly."

## Process note: Desktop ↔ Code review loop

New pattern emerged: Claude Code makes a plan → I bring it to Desktop for review → Desktop updates SYNC.md with feedback → Code reads feedback and executes.

Not always needed, but useful for architectural decisions or non-obvious tradeoffs.

## Tests

43 fast tests passing. `words` array now flows through entire alignment pipeline.

## What's next

**Deferred (for future sessions):**
- Audio duration capture (file vs speech duration)
- Processing stats (filter counts, unknowns merged)
- Language detection from Whisper

**Bigger picture:**
- Test with more recordings
- Story element extraction (characters, worlds, plot beats)
- Story bible / searchable database

---

*Pipeline complete: audio in → JSON out. Now it persists.*
